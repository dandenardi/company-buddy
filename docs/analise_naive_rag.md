# ğŸ” AnÃ¡lise: Company Buddy Ã© um "Naive RAG"?

## ğŸ“Š Resumo Executivo

**Veredito**: **Sim, o Company Buddy atual estÃ¡ na categoria "Naive RAG"** com alguns pontos positivos de multi-tenancy.

**Score Geral**: âš ï¸ **6/7 categorias sÃ£o "Naive"**

---

## 1ï¸âƒ£ IngestÃ£o / Chunking

### ğŸ”¹ Quebra de texto em chunks de tamanho fixo?

**Status**: âœ… **SIM - NAIVE**

**EvidÃªncia**: [document_ingestion.py:41-73](file:///c:/programming/company-buddy/backend/app/services/document_ingestion.py#L41-L73)

```python
def chunk_text(text: str, max_chars: int = 800) -> list[str]:
    """
    Chunk simples baseado em palavras, sem overlap, focado em seguranÃ§a.
    """
    words = text.split()
    chunks: list[str] = []
    current: list[str] = []
    current_length = 0

    for word in words:
        word_len = len(word) + 1
        if current_length + word_len <= max_chars:
            current.append(word)
            current_length += word_len
        else:
            if current:
                chunks.append(" ".join(current))
            current = [word]
            current_length = word_len
```

**Problemas**:

- âŒ Tamanho fixo de 800 caracteres
- âŒ NÃ£o respeita tÃ­tulos, seÃ§Ãµes ou estrutura semÃ¢ntica
- âŒ NÃ£o respeita parÃ¡grafos lÃ³gicos
- âŒ NÃ£o respeita listas ou tabelas
- âŒ Sem overlap entre chunks (pode perder contexto nas bordas)
- âŒ Quebra apenas por palavras (pode cortar no meio de uma ideia)

**O que falta**:

- Chunking semÃ¢ntico (por parÃ¡grafos, seÃ§Ãµes)
- Overlap configurÃ¡vel (ex: 10-20%)
- DetecÃ§Ã£o de estrutura (tÃ­tulos, listas, tabelas)
- Chunking adaptativo baseado no tipo de documento

---

### ğŸ”¹ Sem deduplicaÃ§Ã£o, versionamento ou metadados ricos?

**Status**: âœ… **SIM - NAIVE**

**EvidÃªncia**: [qdrant_service.py:55-89](file:///c:/programming/company-buddy/backend/app/services/qdrant_service.py#L55-L89)

```python
def upsert_chunks(
    self,
    tenant_id: int,
    document_id: int,
    chunks: List[str],
    embeddings: List[List[float]],
) -> None:
    points: List[qmodels.PointStruct] = []
    for idx, (text, vector) in enumerate(zip(chunks, embeddings)):
        point_id = str(uuid4())  # âš ï¸ ID aleatÃ³rio - sem deduplicaÃ§Ã£o
        payload: Dict[str, Any] = {
            "tenant_id": tenant_id,
            "document_id": document_id,
            "chunk_index": idx,
            "text": text,  # âš ï¸ Metadados mÃ­nimos
        }
```

**Problemas**:

- âŒ Sem deduplicaÃ§Ã£o por hash de conteÃºdo
- âŒ Sem controle de versÃµes (se re-upload, cria duplicatas?)
- âŒ Metadados bÃ¡sicos: apenas `tenant_id`, `document_id`, `chunk_index`, `text`

**Metadados que faltam**:

- `document_name` / `filename`
- `content_type` / `mime_type`
- `upload_date` / `created_at`
- `section` / `page_number`
- `document_category` / `tags`
- `language`
- `author` / `department`
- `content_hash` (para deduplicaÃ§Ã£o)
- `version`

**Ponto positivo**: âœ… Tem `tenant_id` (multi-tenancy)

---

## 2ï¸âƒ£ RecuperaÃ§Ã£o

### ğŸ”¹ Apenas top_k por similaridade vetorial, sem filtros, BM25 ou rerank?

**Status**: âœ… **SIM - BEM NAIVE**

**EvidÃªncia**: [qdrant_service.py:91-125](file:///c:/programming/company-buddy/backend/app/services/qdrant_service.py#L91-L125)

```python
def search(self, tenant_id: int, query_text: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Faz busca vetorial filtrada por tenant_id.
    """
    embedding_service = EmbeddingService()
    query_vector = embedding_service.embed_texts([query_text])[0]

    flt = qmodels.Filter(
        must=[
            qmodels.FieldCondition(
                key="tenant_id",
                match=qmodels.MatchValue(value=tenant_id),
            )
        ]
    )

    result = self.client.query_points(
        collection_name=self.collection_name,
        query=query_vector,
        query_filter=flt,  # âš ï¸ Apenas filtro de tenant
        limit=limit,       # âš ï¸ K fixo
        with_payload=True,
        with_vectors=False,
    )
```

**Problemas**:

- âŒ Apenas busca vetorial pura (sem BM25 hÃ­brido)
- âŒ Sem reranking
- âŒ Sem filtros adicionais por metadata (tipo de documento, data, categoria)
- âŒ Sem score threshold (retorna atÃ© chunks irrelevantes)
- âŒ Sem diversidade (pode retornar 5 chunks do mesmo parÃ¡grafo)

**Ponto positivo**: âœ… Filtra por `tenant_id` (isolamento multi-tenant)

---

### ğŸ”¹ K fixo para tudo?

**Status**: âœ… **SIM - NAIVE**

**EvidÃªncia**: [ask.py:22-24](file:///c:/programming/company-buddy/backend/app/api/v1/routes/ask.py#L22-L24)

```python
class AskRequest(BaseModel):
  question: str
  top_k: int = 5  # âš ï¸ Default fixo
```

**Problemas**:

- âš ï¸ K padrÃ£o de 5 para todas as perguntas
- âŒ NÃ£o adapta baseado no tipo de pergunta
- âŒ NÃ£o adapta baseado na complexidade da query

**O que falta**:

- K adaptativo (perguntas simples = menos chunks, complexas = mais)
- Score threshold dinÃ¢mico
- LÃ³gica de "suficiÃªncia" (parar quando tiver contexto suficiente)

---

## 3ï¸âƒ£ GeraÃ§Ã£o / Resposta

### ğŸ”¹ Prompt genÃ©rico sem obrigatoriedade de citar fonte ou formato?

**Status**: âš ï¸ **PARCIALMENTE NAIVE**

**EvidÃªncia**: [llm_service.py:41-61](file:///c:/programming/company-buddy/backend/app/services/llm_service.py#L41-L61)

```python
def answer_with_context(self, question: str, context_chunks: Sequence[str], system_prompt: Optional[str] = None,) -> str:
    context_text = "\n\n".join(context_chunks) if context_chunks else "Nenhum contexto foi encontrado."

    base_prompt = system_prompt or (
        "VocÃª Ã© um assistente interno de uma empresa. "
        "Responda sempre em portuguÃªs brasileiro, de forma clara e objetiva, "
        "usando apenas as informaÃ§Ãµes fornecidas no contexto. "
        "Se nÃ£o encontrar a resposta no contexto, diga que nÃ£o sabe e sugira "
        "que o usuÃ¡rio adicione documentos relacionados."
    )
```

**Pontos positivos**:

- âœ… Instrui a usar apenas o contexto
- âœ… Instrui a dizer "nÃ£o sei" quando nÃ£o encontrar
- âœ… Suporte a `custom_prompt` por tenant ([ask.py:55-62](file:///c:/programming/company-buddy/backend/app/api/v1/routes/ask.py#L55-L62))

**Problemas**:

- âŒ NÃ£o obriga citaÃ§Ã£o de fontes
- âŒ NÃ£o especifica formato de resposta
- âŒ NÃ£o instrui a identificar qual chunk usou
- âš ï¸ Prompt genÃ©rico (mas tem customizaÃ§Ã£o por tenant)

---

### ğŸ”¹ Sem lÃ³gica clara de "nÃ£o sei / nÃ£o encontrado"?

**Status**: âœ… **NÃƒO - TEM LÃ“GICA** (ponto positivo!)

**EvidÃªncia**: [llm_service.py:48-54](file:///c:/programming/company-buddy/backend/app/services/llm_service.py#L48-L54)

```python
base_prompt = system_prompt or (
    "VocÃª Ã© um assistente interno de uma empresa. "
    "Responda sempre em portuguÃªs brasileiro, de forma clara e objetiva, "
    "usando apenas as informaÃ§Ãµes fornecidas no contexto. "
    "Se nÃ£o encontrar a resposta no contexto, diga que nÃ£o sabe e sugira "
    "que o usuÃ¡rio adicione documentos relacionados."
)
```

**Ponto positivo**: âœ… Instrui explicitamente a dizer "nÃ£o sei"

**Problema**: âš ï¸ Depende do LLM seguir a instruÃ§Ã£o (nÃ£o Ã© validaÃ§Ã£o programÃ¡tica)

---

## 4ï¸âƒ£ Conversa e Contexto

### ğŸ”¹ Cada pergunta tratada isoladamente, sem memÃ³ria ou reescrita de query?

**Status**: âœ… **SIM - NAIVE**

**EvidÃªncia**: [ask.py:39-71](file:///c:/programming/company-buddy/backend/app/api/v1/routes/ask.py#L39-L71)

```python
@router.post("", response_model=AskResponse)
def ask(
  payload: AskRequest,
  db: Session = Depends(get_db),
  current_user: UserModel = Depends(get_current_user),
  llm: LLMService = Depends(get_llm_service),
) -> AskResponse:
  # ...
  results = qdrant.search(
    tenant_id=tenant_id,
    query_text=question,  # âš ï¸ Query direta, sem reescrita
    limit=payload.top_k,
  )
```

**Problemas**:

- âŒ Sem memÃ³ria de conversaÃ§Ã£o
- âŒ Sem histÃ³rico de turnos anteriores
- âŒ Sem reescrita de query com base no contexto
- âŒ Perguntas de follow-up nÃ£o funcionam bem

**Exemplo de problema**:

```
User: "Qual a polÃ­tica de fÃ©rias?"
Bot: "30 dias por ano..."
User: "E para estagiÃ¡rios?"  âš ï¸ NÃ£o sabe que Ã© sobre fÃ©rias
```

---

## 5ï¸âƒ£ Observabilidade / Qualidade

### ğŸ”¹ Sem mÃ©tricas de relevÃ¢ncia, satisfaÃ§Ã£o, alucinaÃ§Ã£o ou custo?

**Status**: âœ… **SIM - NAIVE DE PRODUÃ‡ÃƒO**

**EvidÃªncia**: NÃ£o hÃ¡ cÃ³digo de mÃ©tricas, logging de qualidade ou feedback

**Problemas**:

- âŒ Sem mÃ©tricas de relevÃ¢ncia dos chunks retornados
- âŒ Sem feedback do usuÃ¡rio (ğŸ‘/ğŸ‘)
- âŒ Sem detecÃ§Ã£o de alucinaÃ§Ã£o
- âŒ Sem tracking de custo por consulta
- âŒ Sem A/B testing de prompts
- âŒ Sem analytics de queries mais comuns

**Logging bÃ¡sico**: âœ… Tem logs de ingestÃ£o e erros, mas nÃ£o de qualidade

---

## 6ï¸âƒ£ DomÃ­nio / EspecializaÃ§Ã£o

### ğŸ”¹ Mesmo prompt e comportamento para todos os domÃ­nios?

**Status**: âš ï¸ **PARCIALMENTE NAIVE**

**EvidÃªncia**: [ask.py:55-62](file:///c:/programming/company-buddy/backend/app/api/v1/routes/ask.py#L55-L62)

```python
# 0) Busca o tenant para pegar o custom_prompt (se existir)
tenant: TenantModel | None = (
  db.query(TenantModel)
  .filter(TenantModel.id == tenant_id)
  .first()
)

tenant_prompt = tenant.custom_prompt if tenant and tenant.custom_prompt else None
```

**Ponto positivo**: âœ… Suporte a `custom_prompt` por tenant

**Problemas**:

- âš ï¸ CustomizaÃ§Ã£o manual (nÃ£o hÃ¡ templates por domÃ­nio)
- âŒ Sem especializaÃ§Ã£o automÃ¡tica por tipo de documento
- âŒ Sem comportamento diferente para jurÃ­dico vs. tÃ©cnico vs. atendimento

---

## ğŸ“ˆ Scorecard Final

| Categoria           | Status                                | Naive?     |
| ------------------- | ------------------------------------- | ---------- |
| **Chunking**        | Tamanho fixo, sem estrutura semÃ¢ntica | âœ… Sim     |
| **Metadados**       | BÃ¡sicos, sem dedup/versÃ£o             | âœ… Sim     |
| **RecuperaÃ§Ã£o**     | Apenas vetorial, sem BM25/rerank      | âœ… Sim     |
| **K adaptativo**    | K fixo (default 5)                    | âœ… Sim     |
| **Prompt**          | GenÃ©rico, mas customizÃ¡vel            | âš ï¸ Parcial |
| **"NÃ£o sei"**       | Tem instruÃ§Ã£o                         | âŒ NÃ£o     |
| **MemÃ³ria**         | Sem contexto de conversa              | âœ… Sim     |
| **Observabilidade** | Sem mÃ©tricas de qualidade             | âœ… Sim     |
| **EspecializaÃ§Ã£o**  | Custom prompt por tenant              | âš ï¸ Parcial |

**Total Naive**: 6/9 categorias

---

## ğŸ¯ Pontos Fortes (nÃ£o-naive)

1. âœ… **Multi-tenancy robusto** com isolamento por `tenant_id`
2. âœ… **Custom prompt por tenant** (permite especializaÃ§Ã£o)
3. âœ… **InstruÃ§Ã£o de "nÃ£o sei"** no prompt
4. âœ… **Tratamento de erros** do LLM
5. âœ… **Logging bÃ¡sico** de ingestÃ£o

---

## ğŸš¨ Principais Gaps para Evoluir

### Prioridade Alta ğŸ”´

1. **Chunking semÃ¢ntico**

   - Respeitar parÃ¡grafos, seÃ§Ãµes, listas
   - Adicionar overlap (10-20%)
   - Detectar estrutura do documento

2. **Metadados ricos**

   - Adicionar: `filename`, `category`, `upload_date`, `section`, `page`
   - Implementar deduplicaÃ§Ã£o por hash
   - Versionamento de documentos

3. **Reranking**

   - Adicionar modelo de rerank apÃ³s busca vetorial
   - Implementar score threshold
   - Diversidade de resultados

4. **MemÃ³ria de conversa**
   - Armazenar histÃ³rico de turnos
   - Reescrever query com contexto anterior
   - Session management

### Prioridade MÃ©dia ğŸŸ¡

5. **Busca hÃ­brida (BM25 + Vetorial)**

   - Combinar busca lexical e semÃ¢ntica
   - Melhor para nomes prÃ³prios, cÃ³digos, datas

6. **Observabilidade**

   - Feedback do usuÃ¡rio (ğŸ‘/ğŸ‘)
   - MÃ©tricas de relevÃ¢ncia
   - Tracking de custo

7. **K adaptativo**
   - Ajustar baseado no tipo de pergunta
   - Score threshold dinÃ¢mico

### Prioridade Baixa ğŸŸ¢

8. **Templates por domÃ­nio**

   - Prompts especializados (jurÃ­dico, tÃ©cnico, etc.)
   - Comportamento diferente por categoria

9. **CitaÃ§Ã£o de fontes**
   - Obrigar modelo a citar qual chunk usou
   - Link para documento original

---

## ğŸ’¡ RecomendaÃ§Ã£o

**VocÃª estÃ¡ voando no escuro?** âœ… **Sim**

O sistema funciona, mas sem mÃ©tricas de qualidade, vocÃª nÃ£o sabe:

- Se os chunks retornados sÃ£o relevantes
- Se o usuÃ¡rio estÃ¡ satisfeito
- Se hÃ¡ alucinaÃ§Ãµes
- Quanto custa cada consulta

**PrÃ³ximo passo sugerido**: Implementar observabilidade bÃ¡sica (logs de relevÃ¢ncia + feedback do usuÃ¡rio) antes de otimizar chunking/retrieval.

---

## ğŸ“š ReferÃªncias

- [document_ingestion.py](file:///c:/programming/company-buddy/backend/app/services/document_ingestion.py)
- [qdrant_service.py](file:///c:/programming/company-buddy/backend/app/services/qdrant_service.py)
- [llm_service.py](file:///c:/programming/company-buddy/backend/app/services/llm_service.py)
- [ask.py](file:///c:/programming/company-buddy/backend/app/api/v1/routes/ask.py)
